"""
æµ‹è¯•é¡¹ç›®é›†æˆ - CrawlerAgent + trafilatura + ExtractorAgent

éªŒè¯å®Œæ•´çš„çˆ¬å–å’Œæå–æµç¨‹æ˜¯å¦æ­£å¸¸å·¥ä½œ
"""

import asyncio
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.agents import create_crawler_agent, create_extractor_agent
from src.utils import DataManager
from loguru import logger


async def test_full_pipeline():
    """æµ‹è¯•å®Œæ•´çš„çˆ¬å–å’Œæå–æµç¨‹"""

    # æµ‹è¯• URL
    test_url = "http://cpc.people.com.cn/"
    test_keywords = ["é©¬å…‹æ€ä¸»ä¹‰", "ä¸­å›½å…±äº§å…š", "ç†è®º"]

    logger.info("=" * 80)
    logger.info("æµ‹è¯•é¡¹ç›®é›†æˆï¼šCrawlerAgent â†’ trafilatura â†’ ExtractorAgent")
    logger.info("=" * 80)
    logger.info(f"æµ‹è¯• URL: {test_url}")
    logger.info(f"æµ‹è¯•å…³é”®è¯: {test_keywords}")
    logger.info("")

    # åˆ›å»ºæ•°æ®ç®¡ç†å™¨
    data_manager = DataManager(cache_dir=".cache/test", enable_persistence=True)

    # åˆ›å»º CrawlerAgent
    crawler_agent = create_crawler_agent(
        config={
            "timeout": 30,
            "max_retries": 2,
        },
        data_manager=data_manager,
    )

    # åˆ›å»º ExtractorAgent
    extractor_agent = create_extractor_agent(
        config={
            "extract_text_snippet_length": 300,
        },
        data_manager=data_manager,
        keywords=test_keywords,
    )

    try:
        # ============================================================
        # æ­¥éª¤1: CrawlerAgent çˆ¬å–é¡µé¢ï¼ˆä½¿ç”¨ trafilaturaï¼‰
        # ============================================================
        logger.info("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
        logger.info("æ­¥éª¤1: CrawlerAgent çˆ¬å–é¡µé¢")
        logger.info("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")

        crawl_result = await crawler_agent.process({"url": test_url})

        if not crawl_result or not crawl_result.get("success"):
            logger.error(f"âŒ çˆ¬å–å¤±è´¥: {crawl_result.get('error', 'Unknown error')}")
            return

        data_id = crawl_result.get("data_id")
        logger.success(f"âœ… çˆ¬å–æˆåŠŸï¼Œdata_id: {data_id}")
        logger.info(f"   æ ‡é¢˜: {crawl_result.get('title', 'N/A')}")

        # æ£€æŸ¥æ˜¯å¦æœ‰ clean_content
        page_data = data_manager.get_page(data_id)
        if page_data:
            if page_data.clean_content:
                logger.success(f"âœ… trafilatura æå–æˆåŠŸï¼Œæ­£æ–‡é•¿åº¦: {len(page_data.clean_content)} å­—ç¬¦")
                logger.info(f"   æ­£æ–‡é¢„è§ˆï¼ˆå‰300å­—ï¼‰:")
                logger.info(f"   {page_data.clean_content[:300]}...")
            else:
                logger.warning(f"âš ï¸  clean_content ä¸ºç©ºï¼Œtrafilatura å¯èƒ½æå–å¤±è´¥")
                logger.info(f"   markdown é•¿åº¦: {len(page_data.markdown)} å­—ç¬¦")

        # ============================================================
        # æ­¥éª¤2: ExtractorAgent æå–å†…å®¹å’Œå…³é”®è¯
        # ============================================================
        logger.info("")
        logger.info("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
        logger.info("æ­¥éª¤2: ExtractorAgent æå–å†…å®¹å’Œå…³é”®è¯")
        logger.info("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")

        extract_result = extractor_agent.process({
            "data_id": data_id,
            "url": test_url,
            "depth": 0,
            "title": crawl_result.get("title", ""),
        })

        if not extract_result or not extract_result.get("success"):
            logger.error(f"âŒ æå–å¤±è´¥: {extract_result.get('error', 'Unknown error')}")
            return

        logger.success(f"âœ… æå–æˆåŠŸ")
        logger.info(f"   main_content é•¿åº¦: {len(extract_result.get('main_content', ''))} å­—ç¬¦")
        logger.info(f"   headings æ•°é‡: {len(extract_result.get('headings', []))}")
        logger.info(f"   keyword_hits: {extract_result.get('keyword_hits', 0)}")
        logger.info(f"   extracted_links: {extract_result.get('extracted_links_count', 0)}")

        # ============================================================
        # æ­¥éª¤3: è¯¦ç»†ç»“æœåˆ†æ
        # ============================================================
        logger.info("")
        logger.info("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
        logger.info("æ­¥éª¤3: è¯¦ç»†ç»“æœåˆ†æ")
        logger.info("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")

        logger.info(f"\nã€æ ‡é¢˜ã€‘")
        logger.info(f"  {extract_result.get('title', 'N/A')}")

        logger.info(f"\nã€æå–çš„æ ‡é¢˜ï¼ˆheadingsï¼‰ã€‘")
        for i, heading in enumerate(extract_result.get('headings', [])[:10], 1):
            logger.info(f"  {i}. {heading}")

        logger.info(f"\nã€å…³é”®è¯åŒ¹é…ç»Ÿè®¡ã€‘")
        for keyword in test_keywords:
            count = extract_result.get('main_content', '').count(keyword)
            logger.info(f"  '{keyword}': {count} æ¬¡")

        logger.info(f"\nã€æ–‡æœ¬æ‘˜è¦ï¼ˆtext_snippetï¼‰ã€‘")
        logger.info(f"  {extract_result.get('text_snippet', 'N/A')[:300]}...")

        logger.info(f"\nã€å†…å®¹å“ˆå¸Œã€‘")
        logger.info(f"  {extract_result.get('content_hash', 'N/A')}")

        # ============================================================
        # æ­¥éª¤4: è´¨é‡æ£€æŸ¥
        # ============================================================
        logger.info("")
        logger.info("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
        logger.info("æ­¥éª¤4: è´¨é‡æ£€æŸ¥")
        logger.info("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")

        checks = []

        # æ£€æŸ¥1: clean_content æ˜¯å¦å­˜åœ¨
        if page_data and page_data.clean_content:
            checks.append(("âœ…", "trafilatura æå–æˆåŠŸ", True))
        else:
            checks.append(("âŒ", "trafilatura æå–å¤±è´¥", False))

        # æ£€æŸ¥2: main_content é•¿åº¦
        main_content_len = len(extract_result.get('main_content', ''))
        if main_content_len > 500:
            checks.append(("âœ…", f"main_content é•¿åº¦å……è¶³ ({main_content_len} å­—ç¬¦)", True))
        else:
            checks.append(("âŒ", f"main_content é•¿åº¦ä¸è¶³ ({main_content_len} å­—ç¬¦)", False))

        # æ£€æŸ¥3: keyword_hits
        keyword_hits = extract_result.get('keyword_hits', 0)
        if keyword_hits > 0:
            checks.append(("âœ…", f"å…³é”®è¯åŒ¹é…æˆåŠŸ ({keyword_hits} æ¬¡)", True))
        else:
            checks.append(("âš ï¸ ", f"å…³é”®è¯æœªåŒ¹é… (keyword_hits=0)ï¼Œå¯èƒ½æ˜¯è·¨è¯­è¨€å†…å®¹", False))

        # æ£€æŸ¥4: text_snippet è´¨é‡
        text_snippet = extract_result.get('text_snippet', '')
        if text_snippet and len(text_snippet) > 100:
            # æ£€æŸ¥æ˜¯å¦åŒ…å« markdown è¯­æ³•
            has_markdown = any(x in text_snippet for x in ["[](", "**", "__", "# "])
            if not has_markdown:
                checks.append(("âœ…", "text_snippet å¹²å‡€ï¼ˆæ—  markdown æ ‡ç­¾ï¼‰", True))
            else:
                checks.append(("âš ï¸ ", "text_snippet åŒ…å« markdown æ ‡ç­¾", False))
        else:
            checks.append(("âŒ", "text_snippet ä¸ºç©ºæˆ–è¿‡çŸ­", False))

        # æ£€æŸ¥5: æå–é“¾æ¥æ•°é‡
        links_count = extract_result.get('extracted_links_count', 0)
        if links_count > 0:
            checks.append(("âœ…", f"æˆåŠŸæå– {links_count} ä¸ªé“¾æ¥", True))
        else:
            checks.append(("âš ï¸ ", f"æœªæå–åˆ°é“¾æ¥", False))

        # æ‰“å°æ£€æŸ¥ç»“æœ
        all_passed = True
        for icon, message, passed in checks:
            status = "é€šè¿‡" if passed else "å¤±è´¥"
            logger.info(f"  {icon} {message} [{status}]")
            if not passed and icon == "âŒ":
                all_passed = False

        # ============================================================
        # æœ€ç»ˆç»“è®º
        # ============================================================
        logger.info("")
        logger.info("=" * 80)
        if all_passed:
            logger.success("ğŸ‰ æ‰€æœ‰æ£€æŸ¥é€šè¿‡ï¼é¡¹ç›®é›†æˆæ­£å¸¸å·¥ä½œã€‚")
        else:
            logger.warning("âš ï¸  éƒ¨åˆ†æ£€æŸ¥æœªé€šè¿‡ï¼Œéœ€è¦è¿›ä¸€æ­¥æ£€æŸ¥ã€‚")
        logger.info("=" * 80)

    finally:
        # æ¸…ç†èµ„æº
        await crawler_agent.cleanup()

        # æ¸…ç†æµ‹è¯•ç¼“å­˜
        import shutil
        cache_dir = Path(".cache/test")
        if cache_dir.exists():
            shutil.rmtree(cache_dir)
            logger.info("\nå·²æ¸…ç†æµ‹è¯•ç¼“å­˜")


if __name__ == "__main__":
    asyncio.run(test_full_pipeline())
